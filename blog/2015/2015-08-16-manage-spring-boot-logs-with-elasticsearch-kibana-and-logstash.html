<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8">
  <title>Manage Spring Boot Logs with Elasticsearch, Logstash and Kibana</title>
  <meta name="description" content="Manage Spring Boot Logs with Elasticsearch, Logstash and Kibana">
  <meta name="author" content="Krešimir Nesek">
  <meta name="keywords" content="Elasticsearch,Kibana,Logstash,elk,Spring Boot,java">
  <meta name="generator" content="JBake">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link herf="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,600,700,800,300&subset=latin,cyrillic-ext">
  
  <link rel="stylesheet" href="../../elusive-icons/css/elusive-icons.min.css">
  
  <link rel="stylesheet" href="../../css/normalize.css">
  <link rel="stylesheet" href="../../css/skeleton.css">
  <link rel="stylesheet" href="../../css/main.css">

  <link href="../../css/highlight_default.min.css" rel="stylesheet">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>

  <link rel="shortcut icon" href="../../favicon.ico">
  <link rel="icon" type="image/png" href="../../favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="../../favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="../../favicon-16x16.png" sizes="16x16">
</head>
<body> <!-- BODY NAV -->
<div class="navbar-container">
    <div class="container" style="padding-bottom: 6px">
        <div class="navtitle">
             <a href="../../">Teh Artikli</a>
        </div>
        <!-- Nav -->
        <div class="navbar">
            <ul>
<li>
  <a id="nav-archive" href="../../archive.html">
    <i class="el el-calendar"></i>
 <span class="link-label">Archive</span>
  </a>
</li>
<li>
  <a id="nav-subscribe" href="../../feed.xml">
    <i class="el el-rss"></i>
 <span class="link-label">Subscribe</span>
  </a>
</li>
<li>
  <a id="nav-follow" href="https://twitter.com/intent/follow?screen_name=knes1">
    <i class="el el-twitter"></i>
 <span class="link-label">Follow</span>
  </a>
</li>
<li>
  <a id="nav-knes1" href="https://github.com/knes1">
    <i class="el el-github"></i>
 <span class="link-label">knes1</span>
  </a>
</li>
            </ul>
        </div>
    </div>
</div>  <div class="container">
    <div class="row">
      <div class="twelve columns" style="margin-top: 100px">
	
	<div class="page-header">
		<h1>Manage Spring Boot Logs with Elasticsearch, Logstash and Kibana</h1>
	</div>
	
	<div class="post-info underline bottom-pad">
		<i class="el el-calendar"></i>
 16 August 2015
		| <i class="el el-user"></i>
 Krešimir Nesek
	</div>
	
	<div><p>When time comes to deploy a new project, one often overlooked aspect is log management. ELK stack (Elasticsearch, Logstash, Kibana) is, among other things, a powerful and freely available log management solution. In this article I will show you how to install and setup ELK and use it with default log format of a Spring Boot application. </p>
<!-- more --><p>For this guide, I've setup a demo Spring Boot application with logging enabled and with Logstash configuration that will send log entries to Elasticsearch. Demo application is a simple <a href="https://github.com/knes1/todo">todo list</a> available <a href="https://github.com/knes1/todo">here</a>. </p><p><img src="/images/ELK.svg" onerror="this.src='/images/ELK.png'" alt="ELK setup overview"></p><p>Application will store logs into a log file. Logstash will read and parse the log file and ship log entries to an Elasticsearch instance. Finally, we will use Kibana 4 (Elasticsearch web frontend) to search and analyze the logs. </p><h3>Step 1) Install Elasticsearch</h3>
<ul>
  <li>Download elasticsearch zip file from <a href="https://www.elastic.co/downloads/elasticsearch">https://www.elastic.co/downloads/elasticsearch</a></li>
  <li>Extract it to a directory (unzip it)</li>
  <li>Run it (<code>bin/elasticsearch</code> or <code>bin/elasticsearch.bat</code> on Windows)</li>
  <li>Check that it runs using <code>curl -XGET http://localhost:9200</code></li>
</ul><p>Here's how to do it (steps are written for OS X but should be similar on other systems):</p>
<pre><code class="bash">wget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.zip
unzip elasticsearch-1.7.1.zip
cd elasticsearch-1.7.1
bin/elasticsearch
</code></pre><p>Elasticsearch should be running now. You can verify it's running using <code>curl</code>. In a separate terminal window execute a GET request to Elasticsearch's status page:</p>
<pre><code class="bash">curl -XGET http://localhost:9200
</code></pre><p>If all is well, you should get the following result:</p>
<pre><code class="bash">{
  &quot;status&quot; : 200,
  &quot;name&quot; : &quot;Tartarus&quot;,
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;version&quot; : {
    &quot;number&quot; : &quot;1.7.1&quot;,
    &quot;build_hash&quot; : &quot;b88f43fc40b0bcd7f173a1f9ee2e97816de80b19&quot;,
    &quot;build_timestamp&quot; : &quot;2015-07-29T09:54:16Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;4.10.4&quot;
  },
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre><h3>Step 2) Install Kibana 4</h3>
<ul>
  <li>Download Kibana archive from <a href="https://www.elastic.co/downloads/kibana">https://www.elastic.co/downloads/kibana</a>
  <ul>
    <li>Please note that you need to download appropriate distribution for your OS, URL given in examples below is for OS X</li>
  </ul></li>
  <li>Extract the archive</li>
  <li>Run it (<code>bin/kibana</code>)</li>
  <li>Check that it runs by pointing the browser to the Kibana's WebUI</li>
</ul><p>Here's how to do it:</p>
<pre><code class="bash">wget https://download.elastic.co/kibana/kibana/kibana-4.1.1-darwin-x64.tar.gz
tar xvzf kibana-4.1.1-darwin-x64.tar.gz
cd kibana-4.1.1-darwin-x64
bin/kibana
</code></pre><p>Point your browser to <a href="http://localhost:5601">http://localhost:5601</a> (if Kibana page shows up, we're good - we'll configure it later)</p><h3>Step 3) Install Logstash</h3>
<ul>
  <li>Download Logstash zip from <a href="https://www.elastic.co/downloads/logstash">https://www.elastic.co/downloads/logstash</a></li>
  <li>Extract it (unzip it)</li>
</ul>
<pre><code class="bash">wget https://download.elastic.co/logstash/logstash/logstash-1.5.3.zip
unzip logstash-1.5.3.zip
</code></pre><h3>Step 4) Configure Spring Boot's Log File</h3><p>In order to have Logstash ship log files to Elasticsearch, we must first configure Spring Boot to store log entries into a file. We will establish the following pipeline: Spring Boot App → Log File → Logstash → Elasticsearch. There are other ways of accomplishing the same thing, such as configuring logback to use TCP appender to send logs to a remote Logstash instance via TCP, and many other configurations. I prefer the file approach because it's simple, unobtrusive (you can easily add it to existing systems) and nothing will be lost/broken if for some reason Logstash stops working or if Elasticsearch dies.</p><p>Anyhow, let's configure Spring Boot's log file. The simplest way to do this is to configure log file name in <code>application.properties</code>. It's enough to add the following line:</p>
<pre><code class="yaml">logging.file=application.log
</code></pre><p>Spring Boot will now log ERROR, WARN and INFO level messages in the <code>application.log</code> log file and will also rotate it as it reaches 10 Mb.</p><h3>Step 5) Configure Logstash to Understand Spring Boot's Log File Format</h3><p>Now comes the tricky part. We need to create Logstash config file. Typical Logstash config file consists of three main sections: input, filter and output. Each section contains plugins that do relevant part of the processing (such as file input plugin that reads log events from a file or elasticsearch output plugin which sends log events to Elasticsearch).</p><p><img src="/images/logstash-conf.svg" onerror="this.src='/images/logstash-conf.svg'" alt="Logstash config pipeline"></p><p>Input section defines from where Logstash will read input data - in our case it will be a file hence we will use a <code>file</code> plugin with <code>multiline</code> <code>codec</code>, which basically means that our input file may have multiple lines per log entry.</p><h4>Input Section</h4><p>Here's the input section:</p>
<pre><code class="ruby">input {
  file {
    type =&gt; &quot;java&quot;
    path =&gt; &quot;/path/to/application.log&quot;
    codec =&gt; multiline {
      pattern =&gt; &quot;^%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}.*&quot;
      negate =&gt; &quot;true&quot;
      what =&gt; &quot;previous&quot;
    }
  }
}
</code></pre>
<ul>
  <li>We're using <code>file</code> plugin.</li>
  <li><code>type</code> is set to <code>java</code> - it's just additional piece of metadata in case you will use multiple types of log files in the future.</li>
  <li><code>path</code> is the absolute path to the log file. It must be absolute - Logstash is picky about this.</li>
  <li>We're using <code>multiline</code> <code>codec</code> which means that multiple lines may correspond to a single log event,</li>
  <li>In order to detect lines that should logically be grouped with a previous line we use a detection pattern:
  <ul>
    <li><code>pattern =&gt; &quot;^%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}.*&quot;</code> → Each new log event needs to start with date.</li>
    <li><code>negate =&gt; &quot;true&quot;</code> → if it doesn't start with a date ...</li>
    <li><code>what =&gt; &quot;previous&quot;</code> → ... then it should be grouped with a previous line.</li>
  </ul></li>
</ul><p>File input plugin, as configured, will tail the log file (e.g. only read new entries at the end of the file). Therefore, when testing, in order for Logstash to read something you will need to generate new log entries.</p><h4>Filter Section</h4><p>Filter section contains plugins that perform intermediary processing on an a log event. In our case, event will either be a single log line or multiline log event grouped according to the rules described above. In the filter section we will do several things:</p>
<ul>
  <li>Tag a log event if it contains a stacktrace. This will be useful when searching for exceptions later on.</li>
  <li>Parse out (or grok, in logstash terminology) timestamp, log level, pid, thread, class name (logger actually) and log message.</li>
  <li>Specified timestamp field and format - Kibana will use that later for time based searches.</li>
</ul><p>Filter section for Spring Boot's log format that aforementioned things looks like this:</p>
<pre><code class="ruby">filter {
  #If log line contains tab character followed by &#39;at&#39; then we will tag that entry as stacktrace
  if [message] =~ &quot;\tat&quot; {
    grok {
      match =&gt; [&quot;message&quot;, &quot;^(\tat)&quot;]
      add_tag =&gt; [&quot;stacktrace&quot;]
    }
  }

  #Grokking Spring Boot&#39;s default log format
  grok {
    match =&gt; [ &quot;message&quot;, 
               &quot;(?&lt;timestamp&gt;%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- \[(?&lt;thread&gt;[A-Za-z0-9-]+)\] [A-Za-z0-9.]*\.(?&lt;class&gt;[A-Za-z0-9#_]+)\s*:\s+(?&lt;logmessage&gt;.*)&quot;,
               &quot;message&quot;,
               &quot;(?&lt;timestamp&gt;%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- .+? :\s+(?&lt;logmessage&gt;.*)&quot;
             ]
  }

  #Parsing out timestamps which are in timestamp field thanks to previous grok section
  date {
    match =&gt; [ &quot;timestamp&quot; , &quot;yyyy-MM-dd HH:mm:ss.SSS&quot; ]
  }
}
</code></pre><p>Explanation: </p>
<ul>
  <li><code>if [message] =~ &quot;\tat&quot;</code> → If message contains <code>tab</code> character followed by <code>at</code> (this is ruby syntax) then...</li>
  <li>... use the <code>grok</code> plugin to tag stacktraces:
  <ul>
    <li><code>match =&gt; [&quot;message&quot;, &quot;^(\tat)&quot;]</code> → when <code>message</code> matches beginning of the line followed by <code>tab</code> followed by <code>at</code> then...</li>
    <li><code>add_tag =&gt; [&quot;stacktrace&quot;]</code> → ... tag the event with <code>stacktrace</code> tag.</li>
  </ul></li>
  <li>Use the <code>grok</code> plugin for regular Spring Boot log message parsing:
  <ul>
    <li>First pattern extracts timestamp, level, pid, thread, class name (this is actually logger name) and the log message.</li>
    <li>Unfortunately, some log messages don't have logger name that resembles a class name (for example, Tomcat logs) hence the second pattern that will skip the logger/class field and parse out timestamp, level, pid, thread and the log message.</li>
  </ul></li>
  <li>Use <code>date</code> plugin to parse and set the event date:
  <ul>
    <li><code>match =&gt; [ &quot;timestamp&quot; , &quot;yyyy-MM-dd HH:mm:ss.SSS&quot; ]</code> → <code>timestamp</code> field (grokked earlier) contains the timestamp in the specified format</li>
  </ul></li>
</ul><h4>Output Section</h4><p>Output section contains output plugins that send event data to a particular destination. Outputs are the final stage in the event pipeline. We will be sending our log events to stdout (console output, for debugging) and to Elasticsearch.</p><p>Compared to filter section, output section is rather straightforward:</p>
<pre><code class="ruby">output {
  # Print each event to stdout, useful for debugging. Should be commented out in production.
  # Enabling &#39;rubydebug&#39; codec on the stdout output will make logstash
  # pretty-print the entire event as something similar to a JSON representation.
  stdout {
    codec =&gt; rubydebug
  }

  # Sending properly parsed log events to elasticsearch
  elasticsearch {
    host =&gt; &quot;127.0.0.1&quot;
  }
}
</code></pre><p>Explanation:</p>
<ul>
  <li>We are using multiple outputs: <code>stdout</code> and <code>elasticsearch</code>.</li>
  <li><code>stdout { ... }</code> → <code>stdout</code> plugin prints log events to standard output (console).
  <ul>
    <li><code>codec =&gt; rubydebug</code> → Pretty print events using JSON-like format</li>
  </ul></li>
  <li><code>elasticsearch { ... }</code> → <code>elasticsearch</code> plugin sends log events to Elasticsearch server.
  <ul>
    <li><code>host =&gt; &quot;127.0.0.1&quot;</code> → Hostname where Elasticsearch is located - in our case, localhost.</li>
  </ul></li>
</ul>
<blockquote><p><strong>Update 5/9/2016:</strong> At the time of writing this update, the latest versions of Logstash's elasticsearch output plugin uses <code>hosts</code> configuration parameter instead of <code>host</code> which is shown in example above. New parameter takes an array of hosts (e.g. elasticsearch cluster) as value. In other words, if you are using the latest Logstash version, configure elasticsearch output plugin as follows:</p>
  <pre><code class="ruby">elasticsearch {
   hosts =&gt; [&quot;127.0.0.1&quot;]
}
</code></pre>
</blockquote><h4>Putting it all together</h4><p>Finally, the three parts - input, filter and output - need to be copy pasted together and saved into <code>logstash.conf</code> config file. Once the config file is in place and Elasticsearch is running, we can run Logstash:</p>
<pre><code class="bash">/path/to/logstash/bin/logstash -f logstash.conf
</code></pre><p>If everything went well, Logstash is now shipping log events to Elasticsearch.</p><h3>Step 6) Configure Kibana</h3><p>Ok, now it's time to visit the Kibana web UI again. We have started it in step 2 and it should be running at <a href="http://localhost:5601">http://localhost:5601</a>. </p><p>First, you need to point Kibana to Elasticsearch index(s) of your choice. Logstash creates indices with the name pattern of logstash-YYYY.MM.DD. In Kibana Settings → Indices configure the indices:</p>
<ul>
  <li>Index contains time-based events (select this option)</li>
  <li>Use event times to create index names (select this option)</li>
  <li>Index pattern interval: Daily</li>
  <li>Index name or pattern: [logstash-]YYYY.MM.DD</li>
  <li>Click on "Create Index"</li>
</ul><p>Now click on "Discover" tab. In my opinion, "Discover" tab is really named incorrectly in Kibana - it should be labeled as "Search" instead of "Discover" because it allows you to perform new searches and also to save/manage them. Log events should be showing up now in the main window. If they're not, then double check the time period filter in to right corner of the screen. Default table will have 2 columns by default: Time and _source. In order to make the listing more useful, we can configure the displayed columns. From the menu on the left select level, class and logmessage.</p><p><img src="/images/discover-kibana4.png" alt="Kibana 4 Discover Tab" style="max-width: 100%"></p><p>Alright! You're now ready to take control of your logs using ELK stack and start customizing and tweaking your log management configuration. You can download the sample application used when writing this article from here: <a href="https://github.com/knes1/todo">https://github.com/knes1/todo</a>. It's already configured to write logs in a file and has the Logstash config as described above (although absolute paths will need to be tweaked in <code>logstash.conf</code>).</p><p>If you would like to search or follow your EL logs from command line, checkout <a href="http://knes1.github.io/blog/2016/2016-03-06-elktail-command-line-tool-for-tailing-and-querying-ELK-logs.html">Elktail</a> - a command line utility I've created for accessing and tailng logs stored in EL.</p><p>As always, let me know if you have any question/comments or ideas in the comments section below.</p></div>

	<hr />

	<div id="disqus_thread"></div>
	<script type="text/javascript">
	    /* * * CONFIGURATION VARIABLES * * */
	    var disqus_shortname = 'teh-artikli';
		var disqus_identifier = 'blog/2015/2015-08-16-manage-spring-boot-logs-with-elasticsearch-kibana-and-logstash.html';
	    var disqus_url = 'http://knes1.github.io/blog/2015/2015-08-16-manage-spring-boot-logs-with-elasticsearch-kibana-and-logstash.html';
       
	    /* * * DON'T EDIT BELOW THIS LINE * * */
	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

	<script>hljs.initHighlightingOnLoad();</script>	
      </div>
    </div>
  </div>
    <script src="../../js/jquery-1.11.1.min.js"></script>
<script src="//static.getclicky.com/js" type="text/javascript"></script>
<script type="text/javascript">try{ clicky.init(100763555); }catch(e){}</script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100763555ns.gif" /></p></noscript>    <script src="https://platform.twitter.com/widgets.js"></script>
  </body>
</html>